---
title: Machine Learning Basic(1)
date: 2019-08-02 17:45:52
categories:
- Research
- Machine Learning
tags:
- Machine Learning
thumbnail: https://user-images.githubusercontent.com/42334717/62510252-48804500-b849-11e9-938f-07b4d97bb00d.png
---
# 머신러닝의 분류

> 지도학습(레이블이 있는 경우)

+ 사람으로 부터 지도를 받음
+ 분류(classification), 예측(prediction) 모델

`레이블이란 학습 데이터의 속성을 분석하고자 하는 관점에서 정의하는 것`

> 비지도학습(레이블이 없는 경우)

+ 사람으로 지도를 받지 않음
+ 군집(clustering) 모델
  
<!-- more -->

> 강화학습

+ 경우에 따라 지도학습 중 하나로 분류 또는 독립적으로 세 번째 머신러닝 모델로 분류(보통 세 번째 머신러닝으로 분류)
***
# 지도학습

> 분류 모델

+ kNN(k nearest neighbor)
+ Support Vector Machine
+ Decision Tree(의사결정 트리)

> 예측 모델

+ Regression(회귀)

`예측 모델은 일반적으로 회귀 모델을 사용하므로 회귀 모델을 흔히 예측 모델로 인지`

> 분류 vs 예측

+ 결괏값이 데이터세트에 포함된 레이블 중 하나의 값 vs 결괏값이 데이터세트로 결정된 함수식(회귀식)으로 계산한 임의의 값
***
# 상관분석과 회귀분석

> 상관분석

+ 독립변수와 종속변수 간의 관계의 강도
+ 얼마만큼 밀접하게 관련돼있는지를 분석
+ 변수들 간 상관성 유무만 확인, 인과관계는 분석하지 않음

> 회귀분석

+ 관측된 사건을 정량화해서 여러 독립변수와 종속변수의 관계를 함수식으로 설명
+ 종속변수 : 알고 싶어하는 결괏값, 기댓값, 예상값
+ 독립변수 : 결괏값에 영향을 주는 입력값
+ 상관계수(r), 결정계수(r^2)
+ 일반적으로 `r^2 >= 0.65`이면 의미 있는 회귀식
***
# 선형 회귀

+ 선형 : 독립변수가 1차항으로 됨
+ 기하학 관점에서 입력값(독립변수)과 예상값(종속변수)의 관계가 2차원에서는 직선 형태로, 3차원 공간에서는 평면으로 나타남
+ 임의의 변수 x, y 그리고 상수 a에 대해 표현된 함수에서 다음을 만족하면 선형

~~~
f(x+y)=f(x)+f(y)
f(ax)=a*f(x)
~~~
[최소제곱법을 이용한 선형회귀식 구하기(Least squares regression for a straight line)](https://zerohertz.github.io/2018/11/18/Numerical%20analysis(4)/)
***
# 로지스틱 회귀

+ 종속변수가 yes/no과 같은 범주형(categorical)으로 표현될 경우 선형회귀분석 대신 로지스틱 회귀(logistic regression) 분석 방법을 사용
+ 출력값이 [0, 1]을 경계로 결정되는 로지스틱 함수를 회귀식으로 사용
+ 실제로는 예측을 의미하는 회귀분석보다는 분류 모델에 가까움
+ b0, b1을 구하기 위해 최소제곱법 대신 최대가능도법(maximum likelihood method)을 사용

`최대가능도법은 최대가능도추정(MLE; maximum likelihood estimation)이라고도 함`
~~~
g(z)=z/(1+z) 로지스틱(시그모이드) 함수
z=exp(b0+b1*x)
~~~
***
# 빈도론

`확률은 그 사건이 일어난 횟수의 장기적인 비율이다. - John Venn`
+ 확률이론을 바탕으로 얼만큼 빈번하게 특정한 사건이 반복되어 발생하는가를 관찰하고 이를 기반으로 가설을 검증
+ 경험적 사실만을 가지고 판단

`데이터` - `확률 모델` - `가설 검증`
***
# 베이지안론

+ Bayes' Rule 또는 Bayes' Theorem을 기반으로 확률을 추론하는 이론
+ 베이지안 확률론에 의한 추론 모델링 방법은 어떤 가설의 확률을 평가하기 위해 주관적으로 또는 임의적으로 사전 확률을 먼저 정하고 관찰된 데이터를 기반으로 하는 가능도를 계산해서 처음에 설정된 주관적 확률을 보정하는 방법

> Bayes' Rule

~~~
P(H|E)=(P(H)*P(E|H))/P(E)
~~~
`H는 Hypothesis(가설), E는 Evidence(증거)`

~~~
P(H|E)=사후확률(posteriori)
P(H)=사전확률(priori)
P(E|H)=가능도(likelihood)
P(E)=에비던스 모델 또는 정규화 상수(evidence model or normalized constant)
~~~

+ 추론된 가설에 대한 확률(사후확률)을 구하기 위해서는 사전에 주관적 지식으로 예상한 가설에 대한 확률(사전확률)에 관찰된 데이터로 계산한 가능성 정도(가능도 확률)를 곱하고 모든 가설에 대해 증거가 발생될 확률(정규화 상수)로 나누면 됨