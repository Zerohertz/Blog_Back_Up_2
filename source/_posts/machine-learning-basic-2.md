---
title: Machine Learning Basic(2)
date: 2019-08-06 12:55:24
categories:
- Research
- Machine Learning
tags:
- Machine Learning
thumbnail: https://user-images.githubusercontent.com/42334717/62510252-48804500-b849-11e9-938f-07b4d97bb00d.png
---
# kNN 모델(k-Nearest Neighbor)

+ 인스턴스 기반의 러닝(instance-based learning) 또는 메모리 기반의 러닝(memory-based learning)
+ 학습을 사전에 하지 않고 미뤄두고 있다가 새로운 데이터의 태스크 요청이 오면 그때 일반화(분류)를 수행
+ 학습에 필요한 데이터를 메모리에 기억만 하고 있다가 인스턴스가 발생될 때 비로소 일반화하는 일 시작(lazy learning)
+ 새로운 데이터가 어느 그룹에 속하는지 분류하기 위해 그 데이터에 가장 가까이에 있는 학습 데이터가 속한 그룹을 알아봄
+ 위의 작업을 위해 레이블된 학습 데이터들의 특성을 정량화한 후 좌표공간에 표현하는 작업 필요
+ k가 너무 작으면 노이즈에 너무 민감한 오버피팅(overfitting)이 일어나고 k가 너무크면 의사결정을 할 수 있는 경계가 너무 둔감한 언더피팅(underfitting)이 일어남
+ 적절한 k를 찾는게 중요

<!-- more -->
> 특성 2개이면 2차원 평면에 각 학습 데이터가 표현되고 알고자 하는 새로운 데이터를 중심으로 가상의 원(2차원인 경우)을 확장해 가다가 n개의 데이터 중 가장 많은 그룹을 새로운 데이터의 그룹으로 정하는 방법(k=n인 경우)

`kNN에서의 k는 새로운 데이터가 속한 그룹을 알아내기 위한 인접 데이터를 k개만큼 찾겠다는 뜻`
***
# 서포트 벡터 머신(support vector machine)

+ SVM(support vector machine)은 최근까지 가장 보편적으로 사용됐던 분류를 위한 머신러닝 모델
+ 지도학습 모델로 주로 다루고자 하는 데이터가 2개의 그룹으로 분류될 때 사용
+ 학습 데이터가 벡터 공간에 위치한다고 가정(벡터 공간은 직각 좌표계에 학습 데이터가 위치한 공간)
+ SVM은 그러한 벡터 공간에서 학습 데이터가 속한 2개의 그룹을 분류하는 선형 분리자를 찾는 기하학적 모델
+ 차원을 결정하는 요인은 데이터가 가지고 있는 특성(feature)의 수

> SVM의 목표

+ 두 개의 그룹을 분리하는 직선 `y=(w^T)x+b`를 찾는 것
+ `w`는 직선에 수직인 법선 벡터(normal vector)이고 크기는 `||w||`
+ `w`는 직선을 회전 시키는 성질을 가짐
+ `b`는 스칼라 상수이고 `b`값에 따라 직선이 상하좌우로 평행이동함
+ `x`는 임의의 데이터 벡터
+ `(w^T)x`는 스칼라 곱(dot product)인 `w*x`로도 표현할 수 있으며 그 값은 스칼라인 `||w||*||x||cos(theta)`
+ theta는 두 벡터 사이의 각도
+ 두 그룹을 구별하는 직선식을 찾되, 직선식을 사이에 두고 가능하면 두 그룹이 멀리 떨어져 있도록 하는 직선식을 구해야함(더 높은 신뢰도)
***
# 의사결정 트리

+ 귀납적 추론을 기반으로 하는 의사결정 트리는 실무적으로 가장 많이 사용되는 머신러닝 모델 중 하나로 지도학습 모델
+ 주로 불연속 데이터를 다루며 노이즈가 발생해도 중단되거나 엉뚱한 결과를 보여주지 않는 매우 강건한(robust) 모델
+ 의사결정 트리의 알고리즘은 나무를 거꾸로 세운 것과 같이 맨 위쪽에 위치한 루트(root)부터 시작해 줄기(branch), 이파리(leaf) 순서로 하향식 의사결정 구조를 가짐
+ 상황(instance)을 분류하는 부분을 의사결정 노드(decision node)라고 하고 또 다른 줄기를 만드는 분기점이 됨
+ 맨 상위에 있는 의사결정 노드를 루트 노드(root node)라고 함
+ 각 노드를 속성(attribute)이라고 하고 각 속성들은 분기를 결정하는 속성값(attribute value)을 가짐
+ 다른 지도학습 머신러닝 모델과 마찬가지로 알고자 하는 데이터가 소속된 그룹을 알아내는 것을 목적으로 함
+ 스무고개 게임처럼 질문의 답을 반복적으로 이등분하는 방식
***
# 강화학습

+ 주어진 문제만을 지도받고 해결방법은 시행착오를 통해 스스로 찾아냄
+ 인공지능, 제어이론, 신경과학, 운용과학, 행동심리학, 컴퓨터 성능의 발전에 의해 성행
***
# 딥러닝 DNN(Deep Neural Network)

+ 비선형 문제, 다층 신경망에서의 효과적인 학습모델, 역전파 시 지역최솟값의 함정, 신경망의 층 수가 늘어날 때 수렴의 어려움, 문제의 규모가 커질 때 나타나는 컴퓨터 성능의 한계, 레이블된 디지털 데이터의 부족 등이 하나씩 해결된 뒤 성행
+ 2000년대 초 분류나 회귀와 같은 지도학습과 다르게 새로운 데이터를 구별하는 학습 모델을 사람이 사전에 정의하지 않아도 스스로 할 수 있음
+ Deep이라는 말은 신경망의 layer가 많고 각 layer 마다 고려되는 변수가 많다는 뜻(보통 신경망의 layer 수가 10개 이상이면 Very Deep Learning이라고 함)
+ 깊이(layer의 개수)는 hidden layer 개수에 하나를 더하면 됨(hidden layer는 입력층과 출력층 사이에 있는 layer)
***
[참고](https://tensorflow.blog/2016/04/28/first-contact-with-tensorflow/)